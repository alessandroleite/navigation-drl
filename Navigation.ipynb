{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "---\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Starting the Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"env/Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the agents\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. We'll use by default the first available brain to be controlled through Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_env(action):\n",
    "    \n",
    "    env_info = env.step(action)[brain_name]       # sends the action for the environment\n",
    "    next_state = env_info.vector_observations[0]  # get the next state\n",
    "    reward =  env_info.rewards[0]                 # get the reward for the sent action\n",
    "    done = env_info.local_done[0]                 # check if episode has finished\n",
    "    \n",
    "    return next_state, reward, done, env_info\n",
    "    \n",
    "\n",
    "def dqn(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the agent in a given environment following the Deep Q-Learning algorithm and returns the scores.\n",
    "    \n",
    "    Args:\n",
    "        agent (DQNAgent): agent to be trained using this Deep Q-Learning algorithm\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        filename (string) a file name to store the parameters of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list with the scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    scores_window_mean =  []           # the mean of the last 100 episodes\n",
    "    scores_window_std  =  []           # std of the last 100 episodes\n",
    "    epsilon = eps_start                # initialize epsilon\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]           # get the current state\n",
    "        score = 0                                         # initialize the score\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, epsilon)            # selects an action\n",
    "            next_state, reward, done, _ = step_env(action)# sends the action and obtain next_state, reward, \n",
    "                                                          # done, and info\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done) # execute the action\n",
    "            \n",
    "            state = next_state # update the current state\n",
    "            score += reward    # update the score\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        scores_window.append(score)                       # save most recent score\n",
    "        scores.append(score)                              # save most recent score\n",
    "        w_score_mean = np.mean(scores_window)\n",
    "        scores_window_mean.append(w_score_mean)\n",
    "        scores_window_std.append(np.std(scores_window))\n",
    "        \n",
    "        epsilon = max(eps_end, eps_decay * epsilon) # decrease epsilon\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, w_score_mean), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, w_score_mean))     \n",
    "        \n",
    "        if w_score_mean >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, w_score_mean))\n",
    "            \n",
    "            filename = \"checkpoints/{}_checkpoint.pth\".format(type(agent).__name__)\n",
    "            torch.save(agent.qnetwork_local.state_dict(), filename)\n",
    "            break\n",
    "            \n",
    "    return (scores, scores_window_mean, scores_window_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, rolling_mean, rolling_std, title, filename=''):\n",
    "    \n",
    "    \"\"\"Plot the scores and including the moving average\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.axhline(y=13., xmin=0.0, xmax=1.0, color='r', linestyle='--', alpha=0.9, label=\"Score threshold baseline\")\n",
    "    \n",
    "    plt.plot(np.arange(len(scores)), scores);\n",
    "    plt.plot(np.arange(len(rolling_mean)), rolling_mean,label='Moving average');\n",
    "    plt.fill_between(np.arange(len(rolling_mean)), rolling_mean + rolling_std, rolling_mean - rolling_std, \n",
    "                     facecolor='green', alpha=0.4, label=\"Std of the moving average\");\n",
    "    plt.ylabel(\"Score\"); plt.xlabel(\"Episode #\"); plt.title(title);\n",
    "    ax.legend(loc='upper left')\n",
    "    \n",
    "    if filename != '':\n",
    "        fig.savefig(filename, format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Training a DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn.dqn_agent import DQNAgent\n",
    "\n",
    "agent = DQNAgent(state_size, action_size, seed=42)\n",
    "scores_dqn, scores_window_mean_dqn, scores_window_std_dqn = dqn(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(scores_dqn, np.array(scores_window_mean_dqn), np.array(scores_window_std_dqn), \n",
    "            title='DQN', filename='report/figures/dqn_score.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Training a  DDQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn.dqn_agent import DDQNAgent\n",
    "\n",
    "ddqn_agent = DDQNAgent(state_size, action_size, seed=42)\n",
    "scores_ddqn, scores_window_mean_ddqn, scores_window_std_ddqn = dqn(ddqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(scores_ddqn, \n",
    "            np.array(scores_window_mean_ddqn), \n",
    "            np.array(scores_window_std_ddqn), \n",
    "           title='DDQN', filename='report/figures/ddqn_score.pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
